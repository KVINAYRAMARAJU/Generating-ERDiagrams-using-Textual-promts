{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a7b99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vinay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vinay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vinay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\vinay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\vinay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vinay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "import matplotlib.pyplot as mtp  \n",
    "from sklearn import svm,naive_bayes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as mtp  \n",
    "from sklearn import svm,naive_bayes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, sent_tokenize\n",
    "import ast #for updating in attributes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89fd0ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install language-tool-python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8a6826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing for testing to remove spelling mistakes\n",
    "\n",
    "# importing the library  \n",
    "import language_tool_python    \n",
    "# creating the tool  \n",
    "spell_correction = language_tool_python.LanguageTool('en-US')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6349e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255c4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4aeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install graphviz Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e994b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['recruiter', 'hr', 'order', 'patient', 'doctor', 'appointment', 'prescription', 'customer', 'product', 'promotion', 'account', 'transaction', 'loan', 'investment', 'educatee', 'review', 'salesperson', 'position', 'interview', 'interviewee', 'store', 'employee', 'supplier', 'candidate', 'salespeople', 'item', 'part', 'agent', 'teacher', 'ceo', 'project', 'guest', 'rating', 'company', 'expense', 'leave', 'person', 'attendance', 'class', 'manager', 'course', 'university', 'student'])\n"
     ]
    }
   ],
   "source": [
    "# getting attributes\n",
    "from  attributes import domain_dict\n",
    "print(domain_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02bd7b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.358974358974359\n",
      "MultinomialNB()\n"
     ]
    }
   ],
   "source": [
    "# In[67]:\n",
    "\n",
    "\n",
    "# df = pd.read_csv(\"modifications_dataset.csv\")\n",
    "df = pd.read_csv(\"learning_dataset.csv\")\n",
    "\n",
    "\n",
    "# Drop any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "# numerical_data=df.select_dtypes(include=[np.number])\n",
    "# categorical_data=df.select_dtypes(exclude=[np.number])\n",
    "\n",
    "\n",
    "X = df[['Word', 'POS']].apply(lambda x: ' '.join(x), axis=1)\n",
    "y = df['label']\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# Convert the words to a matrix of token counts\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "# Convert the test words to a matrix of token counts\n",
    "X_test_counts = vectorizer.transform(X_test.apply(lambda x: ' '.join(x)))\n",
    "# In[82]:\n",
    "# Train a Naive Bayes classifier\n",
    "clf = MultinomialNB().fit(X_train_counts, y_train)\n",
    "# In[83]:\n",
    "# Use the classifier to predict the labels of the test set\n",
    "y_pred = clf.predict(X_test_counts)\n",
    "# In[84]:\n",
    "# # Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1297acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hr will add positions. recruiter will check positions. recruiter will add candidates. hr will schedule interview. candidate will attend interview. hr will select a candidate \n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "#text= \"In a university, a Student enrolls in Courses. A student must be assigned to at least one or variou other Course. A courses can be assigned to more than one students. Each course is taught by a single Professor. To maintain instruction quality, a Professor can deliver only one course\"\n",
    "#text = \"Employee Provides their attendence Daily. He will be able to apply more than one leave. He can apply for Expenses. He gives their rating. Manager may approve Leave of their employees. She can approve expences applied by Employees.\" \n",
    "#text = input(\"Enter Text\")\n",
    "#text without inventory\n",
    "#main \n",
    "# text = \"A salesperson may manage many other salespeople. A salesperson is be managed by only one salespeople. A salesperson can be an agent for many customers. A customer is managed by one salespeople. A customer can place many orders. An order can be placed by one customer. An order may lists many items. An item may be listed on many orders. An item is assembled from many parts. A part may be assembled into many items. Many employees assemble an item from many parts. A supplier can supply many parts. A part may be supplied by many suppliers\"\n",
    "#text1\n",
    "text = \"HR will add positions. Recruiter will check positions. Recruiter will add candidates. HR will schedule interview. Candidate will attend interview. HR will select a candidate \"\n",
    "#text2\n",
    "#text = \"Employee Provides their attendance Daily. Employees can apply more than one leave. Employees can apply for Expenses. Employees gives their rating. Manager may approve Leave of their employees. manager can approve expenses applied by Employees.\" \n",
    "#text3\n",
    "#text = \"In a university, a Student enrolls in Courses. A student must be assigned to at least one or more Courses. A courses can be assigned to more than one student. Each course is taught by a single Professor. To maintain quality, a Professor can deliver only one course\"\n",
    "#text4 \n",
    "#text = \"HR will add positions. Recruiter will be checking some postions. Recruiter will add add candidates. HR will schedule interview. Candidate will attend interview. HR may select that candidate. \"\n",
    "\n",
    "\n",
    "# remove Spell and grammer correction  \n",
    "#text = spell_correction.correct(text)  \n",
    "\n",
    "#In[65]:\n",
    "\n",
    "text = text.lower()\n",
    "#bad_chars = [';', ':', '!', \"*\",\"@\",\"(\",\")\",\",\",\"be \",\" be\"]\n",
    "bad_chars = [';', ':', '!', \"*\",\"@\",\"(\",\")\",\",\"]\n",
    "\n",
    "# using replace() to\n",
    "# remove bad_chars\n",
    "for i in bad_chars:\n",
    "\ttext = text.replace(i, '')\n",
    "    \n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed9949cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('hr', 'NN'), ('will', 'MD'), ('add', 'VB'), ('position', 'NNS'), ('.', '.')], [('recruiter', 'NN'), ('will', 'MD'), ('check', 'VB'), ('position', 'NNS'), ('.', '.')], [('recruiter', 'NN'), ('will', 'MD'), ('add', 'VB'), ('candidate', 'NNS'), ('.', '.')], [('hr', 'NN'), ('will', 'MD'), ('schedule', 'VB'), ('interview', 'NN'), ('.', '.')], [('candidate', 'NN'), ('will', 'MD'), ('attend', 'VB'), ('interview', 'NN'), ('.', '.')], [('hr', 'NN'), ('will', 'MD'), ('select', 'VB'), ('a', 'DT'), ('candidate', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "array_m = []\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # POS tagging using the Averaged Perceptron Tagger\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Lemmatize each word based on its POS tag\n",
    "    lemmas = []\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in ['NN','NNS', 'NNPS', 'POS']:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "        elif tag in ['RB', 'VBD', 'VBN', 'VBG', 'VB']:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        else:\n",
    "            lemma = word\n",
    "        lemmas.append((lemma, tag))\n",
    "    \n",
    "    array_m.append(lemmas)\n",
    "\n",
    "print(array_m)\n",
    "df_m = pd.DataFrame(array_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae45bda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hr', 'NN', 'Entity'), ('add', 'VB', 'Relation'), ('position', 'NNS', 'Entity'), ('recruiter', 'NN', 'Entity'), ('check', 'VB', 'Relation'), ('position', 'NNS', 'Entity'), ('recruiter', 'NN', 'Entity'), ('add', 'VB', 'Relation'), ('candidate', 'NNS', 'Entity'), ('hr', 'NN', 'Entity'), ('schedule', 'VB', 'Relation'), ('interview', 'NN', 'Entity'), ('candidate', 'NN', 'Entity'), ('attend', 'VB', 'Relation'), ('interview', 'NN', 'Entity'), ('hr', 'NN', 'Entity'), ('select', 'VB', 'Relation'), ('candidate', 'NN', 'Entity')]\n"
     ]
    }
   ],
   "source": [
    "# classification\n",
    "\n",
    "array = []\n",
    "for i in range(df_m.shape[0]):\n",
    "    for j in range(df_m.shape[1]):\n",
    "        word_pos = df_m.iloc[i, j] # Updated indexing with iloc\n",
    "        if word_pos is not None:\n",
    "            word_counts = vectorizer.transform([' '.join(word_pos)])\n",
    "            label = clf.predict(word_counts)[0]\n",
    "            if label != \"O\":\n",
    "                temp = df_m.iloc[i, j] + (label,) # Updated indexing with iloc\n",
    "                array.append(temp)\n",
    "array_list = [tuple(item) for item in array]\n",
    "print(array_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43c63873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hr', 'NN', 'Entity'), ('add', 'VB', 'Relation'), ('position', 'NNS', 'Entity'), ('recruiter', 'NN', 'Entity'), ('check', 'VB', 'Relation'), ('position', 'NNS', 'Entity'), ('recruiter', 'NN', 'Entity'), ('add', 'VB', 'Relation'), ('candidate', 'NNS', 'Entity'), ('hr', 'NN', 'Entity'), ('schedule', 'VB', 'Relation'), ('interview', 'NN', 'Entity'), ('candidate', 'NN', 'Entity'), ('attend', 'VB', 'Relation'), ('interview', 'NN', 'Entity'), ('hr', 'NN', 'Entity'), ('select', 'VB', 'Relation'), ('candidate', 'NN', 'Entity')]\n"
     ]
    }
   ],
   "source": [
    "# code if text has many,more,various make next NN as NNS\n",
    "\n",
    "output = array_list\n",
    "flag = False\n",
    "for i, (word, tag, label) in enumerate(output):\n",
    "    if word in ['many','more','various','numerous','abundant','multiple','several','plenty']:\n",
    "        flag = True\n",
    "        output[i] = (word, 'JJ', 'O')\n",
    "    if flag and label == 'Entity':\n",
    "        output[i] = (word, 'NNS', 'Entity')\n",
    "        flag = False\n",
    "\n",
    "# if 'be' is followed by a relation then delete it\n",
    "for i in range(len(output) - 1):\n",
    "    if output[i][0] in ['be','being','been'] and output[i+1][2] == 'Relation':\n",
    "        output[i] = (output[i][0], 'VB', 'O')\n",
    "print(output)\n",
    "array_list = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5215c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create JSON\n",
    "\n",
    "# # In[86]:\n",
    "\n",
    "# entity_ids = {}\n",
    "# # create the entity_ids dictionary\n",
    "\n",
    "# for i, (word, pos, label) in enumerate(array_list):\n",
    "#     if label == 'Entity' or label == 'Relation':\n",
    "#         entity_ids.setdefault(word, f\"ent_{len(entity_ids) + 1}\")\n",
    "# # create the entities and relations lists\n",
    "\n",
    "# entities = []\n",
    "# attributes = []\n",
    "# relations = []\n",
    "# entity_counter = 1\n",
    "# attribute_counter = 1\n",
    "# relation_counter = 1\n",
    "\n",
    "# for i, (word, pos, label) in enumerate(array_list):\n",
    "#     if label == 'Entity':\n",
    "#         entities.append({\n",
    "#             \"id\": entity_ids[word],\n",
    "#             \"name\": word,\n",
    "#             \"POS\": pos,\n",
    "#             \"properties\": [\n",
    "#                 {\n",
    "#                 \"attribute\": f\"{word}_id\",\n",
    "#                 }\n",
    "#             ]\n",
    "#         })\n",
    "        \n",
    "#     elif label == 'Attribute':\n",
    "#         entities[-1][\"properties\"].append({\n",
    "#             \"attribute\": word\n",
    "#         })\n",
    "         \n",
    "#     elif label == 'Attribute':\n",
    "#         if len(entities) > 0:\n",
    "#                 attribute_name = word.lower()\n",
    "#                 entities[-1]['attributes'].append(attribute_name)\n",
    "#                 attributes.append({\n",
    "#                     \"id\": f\"a{attribute_counter}\",\n",
    "#                     \"name\": attribute_name,\n",
    "#                     \"entity_id\": entities[-1]['id']\n",
    "#                 })\n",
    "#                 attribute_counter += 1\n",
    "        \n",
    "#     elif label == 'Relation':\n",
    "#         relation_name = word\n",
    "#         if len(entities) >= 1:\n",
    "#                 relation_entity_ids = []\n",
    "#                 relation_entity_pos = []\n",
    "#                 for j in range(i-1, -1, -1):  # search backwards for the first entity\n",
    "#                     if array_list[j][2]=='Entity':\n",
    "#                         entity_name = array_list[j][0]\n",
    "#                         if entity_name in entity_ids:\n",
    "#                             relation_entity_ids.append(entity_ids[entity_name])\n",
    "#                             relation_entity_pos.append(array_list[j][1])\n",
    "#                             break\n",
    "#                 for j in range(i+1, len(array_list)):  # search forwards for the second entity\n",
    "#                     if array_list[j][2]=='Entity':\n",
    "#                         entity_name = array_list[j][0]\n",
    "#                         if entity_name in entity_ids:\n",
    "#                             relation_entity_ids.append(entity_ids[entity_name])\n",
    "#                             relation_entity_pos.append(array_list[j][1])\n",
    "#                             break\n",
    "#                 if len(relation_entity_ids) == 2:\n",
    "#                     relations.append({\n",
    "#                         \"id\": f\"r{relation_counter}\",\n",
    "#                         \"name\": relation_name,\n",
    "#                         \"entity_ids\": [\n",
    "#                             {\n",
    "#                                 \"id\":relation_entity_ids[0],\n",
    "#                                 \"pos\":relation_entity_pos[0]\n",
    "#                             },\n",
    "#                             {\n",
    "#                                 \"id\":relation_entity_ids[1],\n",
    "#                                 \"pos\":relation_entity_pos[1]\n",
    "#                             }\n",
    "#                         ]\n",
    "#                     })\n",
    "#                     relation_counter += 1\n",
    "# # create the JSON object\n",
    "# er_diagram = {\n",
    "#     \"entities\": entities,\n",
    "# #     \"attribute\": attributes,\n",
    "#     \"relations\": relations\n",
    "# }\n",
    "\n",
    "# # print the ER diagram as JSON\n",
    "# # print(json.dumps(er_diagram, indent=4))\n",
    "# json_=json.dumps(er_diagram, indent=4)\n",
    "# print(json_)\n",
    "\n",
    "# # In[87]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c16d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code starting to update attribute\n",
    "\n",
    "# to read the existing domain_dict from attributes.py\n",
    "with open(\"attributes.py\", \"r\") as f:\n",
    "    attributes_contents = f.read()\n",
    "\n",
    "# Convert the domain_dict string to a dictionary\n",
    "attributes_dict = ast.literal_eval(attributes_contents.split(\"=\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6bfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67825043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"entities\": [\n",
      "        {\n",
      "            \"id\": \"ent_1\",\n",
      "            \"name\": \"hr\",\n",
      "            \"POS\": \"NN\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_3\",\n",
      "            \"name\": \"position\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_4\",\n",
      "            \"name\": \"recruiter\",\n",
      "            \"POS\": \"NN\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_3\",\n",
      "            \"name\": \"position\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_4\",\n",
      "            \"name\": \"recruiter\",\n",
      "            \"POS\": \"NN\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_6\",\n",
      "            \"name\": \"candidate\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_1\",\n",
      "            \"name\": \"hr\",\n",
      "            \"POS\": \"NN\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_8\",\n",
      "            \"name\": \"interview\",\n",
      "            \"POS\": \"NN\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_6\",\n",
      "            \"name\": \"candidate\",\n",
      "            \"POS\": \"NN\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_8\",\n",
      "            \"name\": \"interview\",\n",
      "            \"POS\": \"NN\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_1\",\n",
      "            \"name\": \"hr\",\n",
      "            \"POS\": \"NN\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_6\",\n",
      "            \"name\": \"candidate\",\n",
      "            \"POS\": \"NN\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"relations\": [\n",
      "        {\n",
      "            \"id\": \"r1\",\n",
      "            \"name\": \"add\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_1\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_3\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r2\",\n",
      "            \"name\": \"check\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_4\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_3\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r3\",\n",
      "            \"name\": \"add\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_4\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_6\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r4\",\n",
      "            \"name\": \"schedule\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_1\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_8\",\n",
      "                    \"pos\": \"NN\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r5\",\n",
      "            \"name\": \"attend\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_6\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_8\",\n",
      "                    \"pos\": \"NN\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r6\",\n",
      "            \"name\": \"select\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_1\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_6\",\n",
      "                    \"pos\": \"NN\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def synonym_antonym_extractor(phrase):\n",
    "    synonyms = []\n",
    "    # Extract synonyms for the given phrase\n",
    "    for syn in wn.synsets(phrase):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    synonyms = set(synonyms)\n",
    "    # Find common keys (entity names) between synonyms and domain_dict\n",
    "    common_keys = synonyms.intersection(domain_dict.keys())\n",
    "    return common_keys\n",
    "\n",
    "# Initialize variables\n",
    "entity_ids = {}\n",
    "entities = []\n",
    "relations = []\n",
    "entity_counter = 1\n",
    "relation_counter = 1\n",
    "new_attributes_dict = {}\n",
    "\n",
    "# Create the entity_ids dictionary\n",
    "for i, (word, pos, label) in enumerate(array_list):\n",
    "    if label == 'Entity' or label == 'Relation':\n",
    "        entity_ids.setdefault(word, f\"ent_{len(entity_ids) + 1}\")\n",
    "\n",
    "for i, (word, pos, label) in enumerate(array_list):\n",
    "    if label == 'Entity':\n",
    "        # Create entity dictionary\n",
    "        entity_dict = {\n",
    "            \"id\": entity_ids[word],\n",
    "            \"name\": word,\n",
    "            \"POS\": pos,\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"attribute\": []\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        entities.append(entity_dict)\n",
    "    elif label == 'Relation':\n",
    "        relation_name = word\n",
    "        if len(entities) >= 1:\n",
    "            relation_entity_ids = []\n",
    "            relation_entity_pos = []\n",
    "            # Find the entity names preceding and succeeding the relation\n",
    "            for j in range(i-1, -1, -1):\n",
    "                if array_list[j][2] == 'Entity':\n",
    "                    entity_name = array_list[j][0]\n",
    "                    if entity_name in entity_ids:\n",
    "                        relation_entity_ids.append(entity_ids[entity_name])\n",
    "                        relation_entity_pos.append(array_list[j][1])\n",
    "                        break\n",
    "            for j in range(i+1, len(array_list)):\n",
    "                if array_list[j][2] == 'Entity':\n",
    "                    entity_name = array_list[j][0]\n",
    "                    if entity_name in entity_ids:\n",
    "                        relation_entity_ids.append(entity_ids[entity_name])\n",
    "                        relation_entity_pos.append(array_list[j][1])\n",
    "                        break\n",
    "            # Create relation dictionary\n",
    "            if len(relation_entity_ids) == 2:\n",
    "                relations.append({\n",
    "                    \"id\": f\"r{relation_counter}\",\n",
    "                    \"name\": relation_name,\n",
    "                    \"entity_ids\": [\n",
    "                        {\n",
    "                            \"id\": relation_entity_ids[0],\n",
    "                            \"pos\": relation_entity_pos[0]\n",
    "                        },\n",
    "                        {\n",
    "                            \"id\": relation_entity_ids[1],\n",
    "                            \"pos\": relation_entity_pos[1]\n",
    "                        }\n",
    "                    ]\n",
    "                })\n",
    "                relation_counter += 1\n",
    "\n",
    "# Create the ER diagram dictionary\n",
    "er_diagram = {\n",
    "    \"entities\": entities,\n",
    "    \"relations\": relations\n",
    "}\n",
    "\n",
    "# Convert the ER diagram dictionary to JSON string\n",
    "json_ = json.dumps(er_diagram, indent=4)\n",
    "print(json_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf38be73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Entities\": [\n",
      "        {\n",
      "            \"id\": \"ent_1\",\n",
      "            \"name\": \"hr\",\n",
      "            \"POS\": \"NN\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_3\",\n",
      "            \"name\": \"position\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_4\",\n",
      "            \"name\": \"recruiter\",\n",
      "            \"POS\": \"NN\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_6\",\n",
      "            \"name\": \"candidate\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_8\",\n",
      "            \"name\": \"interview\",\n",
      "            \"POS\": \"NN\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": []\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"Relationships\": [\n",
      "        {\n",
      "            \"id\": \"r1\",\n",
      "            \"name\": \"add\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_1\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_3\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r2\",\n",
      "            \"name\": \"check\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_4\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_3\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r3\",\n",
      "            \"name\": \"add\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_4\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_6\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r4\",\n",
      "            \"name\": \"schedule\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_1\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_8\",\n",
      "                    \"pos\": \"NN\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r5\",\n",
      "            \"name\": \"attend\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_6\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_8\",\n",
      "                    \"pos\": \"NN\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r6\",\n",
      "            \"name\": \"select\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_1\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_6\",\n",
      "                    \"pos\": \"NN\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Remove entity duplicates\n",
    "entities_dict = {}  # Dictionary to store unique entities\n",
    "\n",
    "json_data = json.loads(json_)  # Convert JSON string to dictionary\n",
    "\n",
    "# Iterate over entities\n",
    "for entity in json_data['entities']:\n",
    "    entity_id = entity['id']\n",
    "    if entity_id in entities_dict:\n",
    "        # Entity already exists, update its properties\n",
    "        existing_entity = entities_dict[entity_id]\n",
    "        existing_properties = existing_entity['properties']\n",
    "        new_properties = entity['properties']\n",
    "        for new_prop in new_properties:\n",
    "            new_attr = new_prop['attribute']\n",
    "            if new_attr not in [prop['attribute'] for prop in existing_properties]:\n",
    "                existing_properties.append(new_prop)\n",
    "    else:\n",
    "        # Add entity to dictionary\n",
    "        entities_dict[entity_id] = entity\n",
    "\n",
    "stored_relations = {}  # Dictionary to store unique relations\n",
    "i = 0\n",
    "\n",
    "while i < len(relations):\n",
    "    relation = relations[i]\n",
    "    duplicate_found = False\n",
    "    highest_degree_pos = {}\n",
    "\n",
    "    if relation['name'] in stored_relations:\n",
    "        duplicate_relation = stored_relations[relation['name']]\n",
    "        if set([entity['id'] for entity in duplicate_relation['entity_ids']]) == set(\n",
    "                [entity['id'] for entity in relation['entity_ids']]):\n",
    "            duplicate_found = True\n",
    "    else:\n",
    "        stored_relations[relation['name']] = relation\n",
    "\n",
    "    if not duplicate_found:\n",
    "        for other_relation in relations:\n",
    "            if other_relation != relation and other_relation['name'] == relation['name']:\n",
    "                if set([entity['id'] for entity in other_relation['entity_ids']]) == set(\n",
    "                        [entity['id'] for entity in relation['entity_ids']]) or \\\n",
    "                   set([entity['id'] for entity in other_relation['entity_ids']]) == set(\n",
    "                        [entity['id'] for entity in relation['entity_ids'][::-1]]):\n",
    "                    for entity in relation['entity_ids']:\n",
    "                        entity_id = entity['id']\n",
    "                        pos = entity['pos']\n",
    "                        if entity_id in highest_degree_pos:\n",
    "                            if pos == 'NNS' and highest_degree_pos[entity_id] == 'NN':\n",
    "                                highest_degree_pos[entity_id] = 'NNS'\n",
    "                        else:\n",
    "                            highest_degree_pos[entity_id] = pos\n",
    "\n",
    "                        other_entity = next(\n",
    "                            (other_entity for other_entity in other_relation['entity_ids'] if\n",
    "                             other_entity['id'] == entity_id), None)\n",
    "                        if other_entity:\n",
    "                            other_pos = other_entity['pos']\n",
    "                            if other_pos != pos:\n",
    "                                if other_pos == 'NNS' and pos == 'NN':\n",
    "                                    highest_degree_pos[entity_id] = 'NNS'\n",
    "\n",
    "                    duplicate_found = True\n",
    "                    break\n",
    "\n",
    "        for entity in relation['entity_ids']:\n",
    "            entity_id = entity['id']\n",
    "            pos = entity['pos']\n",
    "            highest_degree = highest_degree_pos.get(entity_id)\n",
    "            if highest_degree and highest_degree != pos:\n",
    "                entity['pos'] = highest_degree\n",
    "\n",
    "        i += 1\n",
    "    else:\n",
    "        relations.pop(i)\n",
    "\n",
    "seen_relations = set()  # Set to keep track of seen relations\n",
    "i = 0\n",
    "while i < len(relations):\n",
    "    relation = relations[i]\n",
    "    # Check if the relation has been seen before\n",
    "    if (relation['name'], tuple(sorted(e['id'] for e in relation['entity_ids']))) in seen_relations:\n",
    "        relations.pop(i)\n",
    "    else:\n",
    "        seen_relations.add((relation['name'], tuple(sorted(e['id'] for e in relation['entity_ids']))))\n",
    "        i += 1\n",
    "\n",
    "# Convert dictionary back to list of entities\n",
    "entities_list = list(entities_dict.values())\n",
    "\n",
    "# Create the ER diagram dictionary\n",
    "er_diagram = {\n",
    "    \"Entities\": entities_list,\n",
    "    \"Relationships\": relations\n",
    "}\n",
    "\n",
    "# Convert list of entities back to JSON string\n",
    "output_json = json.dumps(er_diagram, indent=4)\n",
    "\n",
    "print(output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bda775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ef3537c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"entities\": [\n",
      "        {\n",
      "            \"id\": \"ent_1\",\n",
      "            \"name\": \"salesperson\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": \"id\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"name\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"address\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"sales_quota\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"job_title\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"phone\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"joining_date\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"salary\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"email\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"sales_region\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_4\",\n",
      "            \"name\": \"agent\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": \"department\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"id\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"name\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"job_title\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"phone\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"address\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"email\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_5\",\n",
      "            \"name\": \"customer\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": \"country\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"name\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"city\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"id\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"state\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"address\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"phone\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"postal_code\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"email\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_7\",\n",
      "            \"name\": \"order\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": \"id\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"total_amount\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"customer_id\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"status\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"product_ids\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_9\",\n",
      "            \"name\": \"item\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": \"id\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"name\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"price\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"description\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_11\",\n",
      "            \"name\": \"part\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": \"id\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"name\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"unit_price\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"vendor\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"description\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_12\",\n",
      "            \"name\": \"employee\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": \"country\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"name\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"city\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"id\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"state\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"address\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"position\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"joining_date\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"phone\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"salary\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"postal_code\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"email\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"ent_13\",\n",
      "            \"name\": \"supplier\",\n",
      "            \"POS\": \"NNS\",\n",
      "            \"properties\": [\n",
      "                {\n",
      "                    \"attribute\": \"country\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"name\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"city\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"id\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"state\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"address\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"products\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"contact_person\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"phone\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"postal_code\"\n",
      "                },\n",
      "                {\n",
      "                    \"attribute\": \"email\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"relations\": [\n",
      "        {\n",
      "            \"id\": \"r1\",\n",
      "            \"name\": \"manage\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_1\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_1\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r3\",\n",
      "            \"name\": \"be\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_1\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_4\",\n",
      "                    \"pos\": \"NN\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r4\",\n",
      "            \"name\": \"manage\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_5\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_1\",\n",
      "                    \"pos\": \"NN\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r5\",\n",
      "            \"name\": \"place\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_5\",\n",
      "                    \"pos\": \"NN\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_7\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r7\",\n",
      "            \"name\": \"list\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_7\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_9\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r9\",\n",
      "            \"name\": \"assemble\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_9\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_11\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r11\",\n",
      "            \"name\": \"assemble\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_12\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_9\",\n",
      "                    \"pos\": \"NN\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"r12\",\n",
      "            \"name\": \"supply\",\n",
      "            \"entity_ids\": [\n",
      "                {\n",
      "                    \"id\": \"ent_13\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"ent_11\",\n",
      "                    \"pos\": \"NNS\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "entities_final = []  # List to store final entities\n",
    "attributes = {}  # Dictionary to store attributes\n",
    "\n",
    "# Extract attributes from existing entities\n",
    "for entity in er_diagram[\"Entities\"]:\n",
    "    word = entity[\"name\"]\n",
    "    entity_attributes = [attr[\"attribute\"] for attr in entity[\"properties\"]]\n",
    "    attributes[word] = entity_attributes\n",
    "\n",
    "# Process entities\n",
    "for word, entity_attributes in attributes.items():\n",
    "    if word in domain_dict.keys():\n",
    "        # Entity attributes found in the domain_dict\n",
    "        entity_final = {\n",
    "            \"id\": entity_ids[word],\n",
    "            \"name\": word,\n",
    "            \"POS\": pos,\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"attribute\": attribute_name\n",
    "                } for attribute_name in dict.fromkeys(domain_dict[word])\n",
    "            ]\n",
    "        }\n",
    "    elif len(synonym_antonym_extractor(phrase=word)) != 0:\n",
    "        # Matching synonyms found in the domain_dict\n",
    "        matched_word = synonym_antonym_extractor(phrase=word)\n",
    "        for value in matched_word:\n",
    "            matched_attr = domain_dict[value]\n",
    "            user_input = input(\"We found these similar attributes: \" + \", \".join(matched_attr) +\n",
    "                               \" for Entity Name: \" + word + \". Do you want to add this (Yes/No):\")\n",
    "            new_attribute = matched_attr\n",
    "            if user_input in [\"yes\", \"Yes\"]:\n",
    "                # Add matched attributes to entity\n",
    "                entity_final = {\n",
    "                    \"id\": entity_ids[word],\n",
    "                    \"name\": word,\n",
    "                    \"POS\": pos,\n",
    "                    \"properties\": [\n",
    "                        {\n",
    "                            \"attribute\": attribute_name\n",
    "                        } for attribute_name in dict.fromkeys(domain_dict[value])\n",
    "                    ]\n",
    "                }\n",
    "                # Update domain_dict with new attributes\n",
    "                if new_attribute:\n",
    "                    new_attributes_dict[word] = set(new_attribute)\n",
    "                    attributes_dict.update(new_attributes_dict)\n",
    "                    # Convert the updated domain_dict back to a string\n",
    "                    new_attributes_contents = f\"domain_dict = {attributes_dict}\"\n",
    "                    # Write the updated domain_dict to attributes.py\n",
    "                    with open(\"attributes.py\", \"w\") as f:\n",
    "                        f.write(new_attributes_contents)\n",
    "                break\n",
    "            else:\n",
    "                input_string = input(\"Unfortunately we didn't find attributes for Entity Name: \" + word +\n",
    "                                     \", Please Enter attributes separated with space:\")\n",
    "                new_attribute = input_string.split()\n",
    "                # Add user-defined attributes to entity\n",
    "                entity_final = {\n",
    "                    \"id\": entity_ids[word],\n",
    "                    \"name\": word,\n",
    "                    \"POS\": pos,\n",
    "                    \"properties\": [\n",
    "                        {\n",
    "                            \"attribute\": attribute\n",
    "                        } for attribute in new_attribute\n",
    "                    ]\n",
    "                }\n",
    "                # Update domain_dict with new attributes\n",
    "                if new_attribute:\n",
    "                    new_attributes_dict[word] = set(new_attribute)\n",
    "                    attributes_dict.update(new_attributes_dict)\n",
    "                    # Convert the updated domain_dict back to a string\n",
    "                    new_attributes_contents = f\"domain_dict = {attributes_dict}\"\n",
    "                    # Write the updated domain_dict to attributes.py\n",
    "                    with open(\"attributes.py\", \"w\") as f:\n",
    "                        f.write(new_attributes_contents)\n",
    "                break\n",
    "    else:\n",
    "        # No attributes found, prompt user for input\n",
    "        input_string = input(\"Unfortunately we didn't find attributes for Entity Name: \" + word +\n",
    "                             \", Please Enter attributes separated with space:\")\n",
    "        new_attribute = input_string.split()\n",
    "        # Add user-defined attributes to entity\n",
    "        entity_final = {\n",
    "            \"id\": entity_ids[word],\n",
    "            \"name\": word,\n",
    "            \"POS\": pos,\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"attribute\": attribute\n",
    "                } for attribute in new_attribute\n",
    "            ]\n",
    "        }\n",
    "        # Update domain_dict with new attributes\n",
    "        if new_attribute:\n",
    "            new_attributes_dict[word] = set(new_attribute)\n",
    "            attributes_dict.update(new_attributes_dict)\n",
    "            # Convert the updated domain_dict back to a string\n",
    "            new_attributes_contents = f\"domain_dict = {attributes_dict}\"\n",
    "            # Write the updated domain_dict to attributes.py\n",
    "            with open(\"attributes.py\", \"w\") as f:\n",
    "                f.write(new_attributes_contents)\n",
    "    entities_final.append(entity_final)\n",
    "\n",
    "er_diagram = {\n",
    "    \"entities\": entities_final,\n",
    "    \"relations\": relations\n",
    "}\n",
    "\n",
    "# Print the ER diagram as JSON\n",
    "json_final = json.dumps(er_diagram, indent=4)\n",
    "print(json_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42f57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef63dcc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #checking for SYNONYMS in corpus \n",
    "# def synonym_antonym_extractor(phrase):\n",
    "#     synonyms = []\n",
    "#     for syn in wn.synsets(phrase):\n",
    "#         for l in syn.lemmas():\n",
    "#             synonyms.append(l.name())\n",
    "#     synonyms = set(synonyms)\n",
    "#     common_keys = synonyms.intersection(domain_dict.keys())\n",
    "#     return common_keys\n",
    "\n",
    "# #create JSON\n",
    "\n",
    "# entity_ids = {}\n",
    "# # create the entity_ids dictionary\n",
    "# for i, (word, pos, label) in enumerate(array_list):\n",
    "#     if label == 'Entity' or label == 'Relation':\n",
    "#         entity_ids.setdefault(word, f\"ent_{len(entity_ids) + 1}\")\n",
    "\n",
    "# # create the entities and relations lists\n",
    "# entities = []\n",
    "# attributes = []\n",
    "# relations = []\n",
    "# entity_counter = 1\n",
    "# attribute_counter = 1\n",
    "# relation_counter = 1\n",
    "\n",
    "# new_attributes_dict = {}\n",
    "\n",
    "# for i, (word, pos, label) in enumerate(array_list):\n",
    "#     if label == 'Entity':\n",
    "#         if word in domain_dict.keys():\n",
    "#             entity_dict = {\n",
    "#                 \"id\": entity_ids[word],\n",
    "#                 \"name\": word,\n",
    "#                 \"POS\": pos,\n",
    "#                 \"properties\": [\n",
    "#                     {\n",
    "#                         \"attribute\": attribute_name\n",
    "#                     } for attribute_name in dict.fromkeys(domain_dict[word])\n",
    "#                 ]\n",
    "#             }\n",
    "#         elif (len(synonym_antonym_extractor(phrase= word)) != 0):\n",
    "#             matched_word = synonym_antonym_extractor(phrase= word)\n",
    "#             for value in matched_word:\n",
    "#                 matched_attr = domain_dict[value]\n",
    "#                 user_input = input(\"We found these similar attributes: \"+ matched_attr +\", for Entity Name: \"+ word +\". Do you want to add this(Yes/No):\")\n",
    "#                 new_attribute = matched_attr\n",
    "#                 if user_input == \"Yes\":\n",
    "#                     entity_dict = {\n",
    "#                     \"id\": entity_ids[word],\n",
    "#                     \"name\": word,\n",
    "#                     \"POS\": pos,\n",
    "#                     \"properties\": [\n",
    "#                         {\n",
    "#                             \"attribute\": attribute_name\n",
    "#                         } for attribute_name in dict.fromkeys(domain_dict[value])\n",
    "#                     ]\n",
    "#                 }\n",
    "#                     # do not add null set to dictionary\n",
    "#                     if new_attribute:\n",
    "#                         new_attributes_dict[word] = set(new_attribute)\n",
    "#                         attributes_dict.update(new_attributes_dict)\n",
    "#                         # Convert the updated domain_dict back to a string\n",
    "#                         new_attributes_contents = f\"domain_dict = {attributes_dict}\"\n",
    "#                         # Write the updated domain_dict to attributes.py\n",
    "#                         with open(\"attributes.py\", \"w\") as f:\n",
    "#                             f.write(new_attributes_contents)\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     input_string = input(\"Unfortunately we didn't find attributes for Entity Name: \"+ word +\", Please Enter attributes separated with space:\" )\n",
    "#                     new_attribute = input_string.split()\n",
    "#                     entity_dict = {\n",
    "#                     \"id\": entity_ids[word],\n",
    "#                     \"name\": word,\n",
    "#                     \"POS\": pos,\n",
    "#                     \"properties\": [\n",
    "#                         {\n",
    "#                             \"attribute\":attribute\n",
    "#                         } for attribute in new_attribute\n",
    "#                     ]\n",
    "#                 }\n",
    "#                     if new_attribute:\n",
    "#                         new_attributes_dict[word] = set(new_attribute)\n",
    "#                         attributes_dict.update(new_attributes_dict)\n",
    "#                         # Convert the updated domain_dict back to a string\n",
    "#                         new_attributes_contents = f\"domain_dict = {attributes_dict}\"\n",
    "#                         # Write the updated domain_dict to attributes.py\n",
    "#                         with open(\"attributes.py\", \"w\") as f:\n",
    "#                             f.write(new_attributes_contents)\n",
    "#                     break\n",
    "            \n",
    "#         else:\n",
    "#             input_string = input(\"Unfortunately we didn't find attributes for Entity Name: \"+ word +\", Please Enter attributes separated with space:\" )\n",
    "#             new_attribute = input_string.split()\n",
    "#             entity_dict = {\n",
    "#                 \"id\": entity_ids[word],\n",
    "#                 \"name\": word,\n",
    "#                 \"POS\": pos,\n",
    "#                 \"properties\": [\n",
    "#                     {\n",
    "#                         \"attribute\":attribute\n",
    "#                     } for attribute in new_attribute\n",
    "#                 ]\n",
    "#             }\n",
    "#             if new_attribute:\n",
    "#                         new_attributes_dict[word] = set(new_attribute)\n",
    "#                         attributes_dict.update(new_attributes_dict)\n",
    "#                         # Convert the updated domain_dict back to a string\n",
    "#                         new_attributes_contents = f\"domain_dict = {attributes_dict}\"\n",
    "#                         # Write the updated domain_dict to attributes.py\n",
    "#                         with open(\"attributes.py\", \"w\") as f:\n",
    "#                             f.write(new_attributes_contents)         \n",
    "#         entities.append(entity_dict)\n",
    "        \n",
    "#     elif label == 'Attribute':\n",
    "#         entities[-1][\"properties\"].append({\n",
    "#             \"attribute\": word\n",
    "#         })\n",
    "         \n",
    "#     elif label == 'Relation':\n",
    "#         relation_name = word\n",
    "#         if len(entities) >= 1:\n",
    "#                 relation_entity_ids = []\n",
    "#                 relation_entity_pos = []\n",
    "#                 for j in range(i-1, -1, -1):  # search backwards for the first entity\n",
    "#                     if array_list[j][2]=='Entity':\n",
    "#                         entity_name = array_list[j][0]\n",
    "#                         if entity_name in entity_ids:\n",
    "#                             relation_entity_ids.append(entity_ids[entity_name])\n",
    "#                             relation_entity_pos.append(array_list[j][1])\n",
    "#                             break\n",
    "#                 for j in range(i+1, len(array_list)):  # search forwards for the second entity\n",
    "#                     if array_list[j][2]=='Entity':\n",
    "#                         entity_name = array_list[j][0]\n",
    "#                         if entity_name in entity_ids:\n",
    "#                             relation_entity_ids.append(entity_ids[entity_name])\n",
    "#                             relation_entity_pos.append(array_list[j][1])\n",
    "#                             break\n",
    "#                 if len(relation_entity_ids) == 2:\n",
    "#                     relations.append({\n",
    "#                         \"id\": f\"r{relation_counter}\",\n",
    "#                         \"name\": relation_name,\n",
    "#                         \"entity_ids\": [\n",
    "#                             {\n",
    "#                                 \"id\":relation_entity_ids[0],\n",
    "#                                 \"pos\":relation_entity_pos[0]\n",
    "#                             },\n",
    "#                             {\n",
    "#                                 \"id\":relation_entity_ids[1],\n",
    "#                                 \"pos\":relation_entity_pos[1]\n",
    "#                             }\n",
    "#                         ]\n",
    "#                     })\n",
    "#                     relation_counter += 1\n",
    "                    \n",
    "# # create the JSON object\n",
    "# er_diagram = {\n",
    "#     \"entities\": entities,\n",
    "#     \"relations\": relations\n",
    "# }\n",
    "\n",
    "# # print the ER diagram as JSON\n",
    "# json_=json.dumps(er_diagram, indent=4)\n",
    "# print(json_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96288b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove entity duplicates\n",
    "# entities_dict = {}\n",
    "\n",
    "# json_data = json.loads(json_)\n",
    "\n",
    "# for entity in json_data['entities']:\n",
    "#     entity_id = entity['id']\n",
    "#     if entity_id in entities_dict:\n",
    "#         # Entity already exists, update its properties\n",
    "#         existing_entity = entities_dict[entity_id]\n",
    "#         existing_properties = existing_entity['properties']\n",
    "#         new_properties = entity['properties']\n",
    "#         for new_prop in new_properties:\n",
    "#             new_attr = new_prop['attribute']\n",
    "#             if new_attr not in [prop['attribute'] for prop in existing_properties]:\n",
    "#                 existing_properties.append(new_prop)\n",
    "#     else:\n",
    "#         # Add entity to dictionary\n",
    "#         entities_dict[entity_id] = entity\n",
    "\n",
    "# # remove duplicates in relations\n",
    "# # to print cardinality without overriding\n",
    "\n",
    "\n",
    "# stored_relations = {}\n",
    "# i = 0\n",
    "\n",
    "# while i < len(relations):\n",
    "#     relation = relations[i]\n",
    "#     duplicate_found = False\n",
    "#     highest_degree_pos = {}\n",
    "\n",
    "#     if relation['name'] in stored_relations:\n",
    "#         duplicate_relation = stored_relations[relation['name']]\n",
    "#         if set([entity['id'] for entity in duplicate_relation['entity_ids']]) == set([entity['id'] for entity in relation['entity_ids']]):\n",
    "#             duplicate_found = True\n",
    "#     else:\n",
    "#         stored_relations[relation['name']] = relation\n",
    "\n",
    "#     if not duplicate_found:\n",
    "#         for other_relation in relations:\n",
    "#             if other_relation != relation and other_relation['name'] == relation['name']:\n",
    "#                 if set([entity['id'] for entity in other_relation['entity_ids']]) == set([entity['id'] for entity in relation['entity_ids']]) or \\\n",
    "#                    set([entity['id'] for entity in other_relation['entity_ids']]) == set([entity['id'] for entity in relation['entity_ids'][::-1]]):\n",
    "#                     for entity in relation['entity_ids']:\n",
    "#                         entity_id = entity['id']\n",
    "#                         pos = entity['pos']\n",
    "#                         if entity_id in highest_degree_pos:\n",
    "#                             if pos == 'NNS' and highest_degree_pos[entity_id] == 'NN':\n",
    "#                                 highest_degree_pos[entity_id] = 'NNS'\n",
    "#                         else:\n",
    "#                             highest_degree_pos[entity_id] = pos\n",
    "\n",
    "#                         other_entity = next((other_entity for other_entity in other_relation['entity_ids'] if other_entity['id'] == entity_id), None)\n",
    "#                         if other_entity:\n",
    "#                             other_pos = other_entity['pos']\n",
    "#                             if other_pos != pos:\n",
    "#                                 if other_pos == 'NNS' and pos == 'NN':\n",
    "#                                     highest_degree_pos[entity_id] = 'NNS'\n",
    "\n",
    "#                     duplicate_found = True\n",
    "#                     break\n",
    "\n",
    "#         for entity in relation['entity_ids']:\n",
    "#             entity_id = entity['id']\n",
    "#             pos = entity['pos']\n",
    "#             highest_degree = highest_degree_pos.get(entity_id)\n",
    "#             if highest_degree and highest_degree != pos:\n",
    "#                 entity['pos'] = highest_degree\n",
    "\n",
    "#         i += 1\n",
    "#     else:\n",
    "#         relations.pop(i)\n",
    "\n",
    "# seen_relations = set()\n",
    "# i = 0\n",
    "# while i < len(relations):\n",
    "#     relation = relations[i]\n",
    "#     if (relation['name'], tuple(sorted(e['id'] for e in relation['entity_ids']))) in seen_relations:\n",
    "#         relations.pop(i)\n",
    "#     else:\n",
    "#         seen_relations.add((relation['name'], tuple(sorted(e['id'] for e in relation['entity_ids']))))\n",
    "#         i += 1\n",
    "\n",
    "\n",
    "# # # Iterate over each relation and check for duplicates\n",
    "# # i = 0\n",
    "# # while i < len(relations):\n",
    "# #     relation = relations[i]\n",
    "# #     duplicate_found = False\n",
    "    \n",
    "# #     # Check for duplicates\n",
    "# #     for other_relation in relations:\n",
    "# #         if other_relation != relation and other_relation['name'] == relation['name']:\n",
    "# #             if set([entity['id'] for entity in other_relation['entity_ids']]) == set([entity['id'] for entity in relation['entity_ids']]):\n",
    "# #                 duplicate_found = True\n",
    "# #                 break\n",
    "    \n",
    "# #     # Remove the duplicate if found\n",
    "# #     if duplicate_found:\n",
    "# #         relations.pop(i)\n",
    "# #     else:\n",
    "# #         i += 1\n",
    "\n",
    "\n",
    "\n",
    "# # Convert dictionary back to list of entities\n",
    "# entities_list = list(entities_dict.values())\n",
    "# er_diagram = {\n",
    "#     \"Entities\": entities_list,\n",
    "#     \"Relationships\": relations\n",
    "# }\n",
    "\n",
    "# # Convert list of entities back to JSON string\n",
    "# output_json = json.dumps(er_diagram, indent=4)\n",
    "\n",
    "# print(output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2147a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code to remove non related entities\n",
    "# data = json.loads(output_json)\n",
    "\n",
    "# entity_ids = [entity[\"id\"] for entity in data[\"Entities\"]]\n",
    "# relationship_entity_ids = [entity[\"id\"] for relationship in data[\"Relationships\"] for entity in relationship[\"entity_ids\"]]\n",
    "\n",
    "# for entity in data[\"Entities\"]:\n",
    "#     if entity[\"id\"] not in relationship_entity_ids:\n",
    "#         data[\"Entities\"].remove(entity)\n",
    "\n",
    "# output_json = json.dumps(data, indent=4)\n",
    "# print(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "340e9ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alligned graph\n",
    "\n",
    "# Load the ER diagram JSON data\n",
    "er_diagram = json.loads(json_final)\n",
    "\n",
    "# Create a Graphviz graph with left-to-right direction\n",
    "graph = Digraph(graph_attr={'rankdir': 'LR'})\n",
    "\n",
    "# Add the entities and their properties as nodes to the graph\n",
    "for entity in er_diagram['entities']:\n",
    "    entity_id = entity['id']\n",
    "    graph.node(entity_id, entity['name'], shape='rectangle', style='filled', fillcolor='lightblue')\n",
    "\n",
    "    for property in entity['properties']:\n",
    "        node_id = f\"{entity_id}_{property['attribute']}\"\n",
    "        graph.node(node_id, property['attribute'], shape='oval')\n",
    "        graph.edge(entity_id, node_id, dir='none')\n",
    "\n",
    "# Add the relations and their entities as nodes to the graph\n",
    "for relation in er_diagram['relations']:\n",
    "    relation_id = relation['id']\n",
    "    graph.node(relation_id, relation['name'], shape='diamond', style='filled', fillcolor='lightgrey')\n",
    "\n",
    "    entity_ids = relation['entity_ids']\n",
    "    from_id, to_id = entity_ids[0]['id'], entity_ids[1]['id']\n",
    "    l1 = '1' if entity_ids[0]['pos'] == 'NN' else 'm'\n",
    "    l2 = '1' if entity_ids[1]['pos'] == 'NN' else 'm'\n",
    "#     l2 = '1' if entity_ids[0]['pos'] in ['NNS','NNP'] else 'm'\n",
    "    graph.edge(from_id, relation_id, dir='none',label = l1)\n",
    "    graph.edge(relation_id, to_id, dir='none',label = l2)\n",
    "    \n",
    "# Render the Graphviz graph and save it as a PNG image\n",
    "graph.render('er_diagram', format='png')\n",
    "\n",
    "# Open the image using PIL library\n",
    "im = Image.open('er_diagram.png')\n",
    "\n",
    "# Display the image in the console\n",
    "im.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e94c23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the dataset with a new word\n",
    "def update_dataset(word, pos, label, df):\n",
    "    # Check if the word and pos already exist in the dataset\n",
    "    if df[(df['Word'] == word) & (df['POS'] == pos)].empty:\n",
    "        # If not, create a new dataframe with the new row\n",
    "        new_row = pd.DataFrame({'Word': [word], 'POS': [pos], 'label': [label]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    else:\n",
    "        # If the word and pos already exist in the dataset, update the label\n",
    "        df.loc[(df['Word'] == word) & (df['POS'] == pos), 'label'] = label\n",
    "    # Write the updated dataframe to the 'learning_dataset.csv' file\n",
    "    df.to_csv('learning_dataset.csv', index=False)\n",
    "    # Return the updated dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9866b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is the ERD(good/bad): good\n",
      "I sincerely appreciate your feedback and will use it to enhance our performance. Thank you!\n"
     ]
    }
   ],
   "source": [
    "#interactive learning\n",
    "\n",
    "# Iterate over the list of tuples\n",
    "feedback = input(\"How is the ERD(good/bad): \")\n",
    "\n",
    "if feedback == \"good\":\n",
    "    for word, pos, label in array_list:\n",
    "        # Call the update_dataset function for every word\n",
    "#         print(word,pos,label)\n",
    "        df = pd.read_csv(\"learning_dataset.csv\")\n",
    "        update_dataset(word, pos, label, df)\n",
    "    print(\"I sincerely appreciate your feedback and will use it to enhance our performance. Thank you!\")\n",
    "elif feedback == \"bad\":\n",
    "    for word, pos, label in array_list:\n",
    "        # Call the update_dataset function for every word\n",
    "#         print(word, pos, label)\n",
    "        df = pd.read_csv(\"learning_dataset.csv\")\n",
    "        update_dataset(word, pos, label, df)\n",
    "    # Take multiple words, their POS, and updated labels as input from the user\n",
    "    words = input(\"Enter words separated by space: \").split(' ')\n",
    "    poss = input(\"Enter POS tags separated by space: \").split(' ')\n",
    "    labels = input(\"Enter labels separated by space: \").split(' ')\n",
    "    for i in range(len(words)):\n",
    "        df = pd.read_csv(\"learning_dataset.csv\")\n",
    "        if ((df['Word'] == words[i]) & (df['POS'] == poss[i])).any():\n",
    "            update_dataset(words[i], poss[i], labels[i], df)\n",
    "        else:\n",
    "            print(\"I'm sorry, but the words you mentioned do not appear in the sentence. Could you please double-check?\")\n",
    "    print(\"Thank you for your valuable feedback. It helps me improve.\")\n",
    "else:\n",
    "    print(\"Please provide your feedback for me to improve.\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eed5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing code to lable m at relations of NNS\n",
    "\n",
    "# er_diagram = json.loads(output_json)\n",
    "\n",
    "# # Create a Graphviz graph\n",
    "# graph = Digraph()\n",
    "# # graph = Digraph(graph_attr={'rankdir': 'LR'})\n",
    "\n",
    "# # sample set to add all the edges. This helps to avoid repeted edges\n",
    "# added_edges = set()\n",
    "\n",
    "# # Add the entities and their properties as nodes to the graph\n",
    "# for entity in er_diagram['Entities']:\n",
    "#     graph.node(entity['id'], entity['name'], shape='rectangle')\n",
    "\n",
    "#     for property in entity['properties']:\n",
    "#         node_id = f\"{entity['id']}_{property['attribute']}\"\n",
    "#         graph.node(node_id, property['attribute'])\n",
    "#         edge = (entity['id'], node_id)\n",
    "#         if edge not in added_edges:\n",
    "#             graph.edge(*edge, dir=\"none\")\n",
    "#             added_edges.add(edge)\n",
    "\n",
    "# # Add the relations and their entities as nodes to the graph\n",
    "# for relation in er_diagram['Relationships']:\n",
    "#     graph.node(relation['id'], relation['name'], shape='diamond')\n",
    "#     for entity in relation['entity_ids']:\n",
    "#         edge = (entity['id'], relation['id'])\n",
    "#         if edge not in added_edges:\n",
    "#             if 'pos' in entity and entity['pos'] in ['NNS','NNP']:\n",
    "#                 graph.edge(*edge, dir=\"none\", label='m')\n",
    "#                 added_edges.add(edge)\n",
    "#             elif 'pos' in entity and entity['pos'] == 'NN':\n",
    "#                 graph.edge(*edge, dir=\"none\", label='1')\n",
    "#                 added_edges.add(edge)\n",
    "#             else:\n",
    "#                 graph.edge(*edge, dir=\"none\")\n",
    "#                 added_edges.add(edge)\n",
    "            \n",
    "# # for relation in er_diagram['Relationships']:\n",
    "# #     if 'entity_ids' in relation:\n",
    "# #         for entity_id in relation['entity_ids']:\n",
    "# #             if 'pos' in entity_id and entity_id['pos'] == 'NNS':\n",
    "# #                 relation['label'] = 'm'\n",
    "# #                 break\n",
    "\n",
    "\n",
    "# # Render the Graphviz graph and save it as a PNG image\n",
    "# graph.render('er_diagram', format='png')\n",
    "\n",
    "# # Open the image using PIL\n",
    "# im = Image.open('er_diagram.png')\n",
    "\n",
    "# # Display the image in the console\n",
    "# im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209f200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3581abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
